\ifx \globalmark \undefined %% This is default.
	\input{../header}
	\begin{document} %% Crashes if put after (one of the many mysteries of LaTeX?).
\else
\fi




\chapter{Basic models} \label{chap:Models}
{\large{\itshape
``Let's play a game!''} --- The puppet guy from the Saw movies.\\
}
{\small{\itshape
The chapter is based on \cite[pages 37 to 74]{MyGTAO}.}\\
}


In this chapter, we begin our analysis of \emph{games}, that are \emph{situations where the decision of individuals (called players) impact the welfare of each other.}

Our goal here is to present the two principal tools to \emph{model} games within well-defined mathematical frameworks.
This allows us to discuss and analyze central concepts, such as the notions of \emph{best-responses} and \emph{domination}, and in doing so,
gain a finer understanding of how we can expect rational players to behave in a game.



\section{The extensive form and the strategic form}

We first present tools that allow to provide a mathematical description from a game out of a high-level description, to a mathematical description of this game.
In order to introduce the models, we begin, in Example \ref{chap2:example:game}, by describing a game to be modeled and analyzed later.
\begin{example}[The betting game]
\label{chap2:example:game}
\TAtwo{} asks \TAone{} to play a game. He explains the rules as follows:
\begin{enumerate}
\item Both players put $1\EUR$ on a table. \label{ex1}
\item  \TAtwo{} picks a card randomly out of a regular deck of 52 cards. He looks at the card privately. \label{ex2}
\item At this point, two options are available to him: \label{ex3}
	\begin{enumerate}
	\item He reveals the card to $\TAone{}$. If the card is \emph{red}, $\TAtwo{}$ gets all the money on the table, and if \emph{black}, $\TAone{}$ gets all the money. \label{ex3a}
	\item He chooses not to reveal the card, and raises the bet by putting an additional  $1\EUR$ on the table. In this case, then $\TAone{}$ has to decide to either \label{ex3b}
	\begin{enumerate}
		\item Fold, letting $\TAone{}$ win the game, or \label{ex3b1}
		\item meet the raise, adding $1\EUR$ on the table. In this case, the card is revealed, and $\TAtwo{}$ wins everything if it is \emph{red}, or $\TAone{}$ wins if it is \emph{black}. \label{ex3b2}
	\end{enumerate}
	In any case, \textbf{the game ends here}.
	\end{enumerate}
\end{enumerate}
\end{example}
At this point, we would like to encourage the reader to reflect on the game of Example \ref{chap2:example:game}.
In particular, we are interested by the following questions
\begin{itemize}
	\item What should $\TAtwo{}$ do in the situation \ref{ex3}? When should he reveal the card, and when should he raise the bet?
	\item What should $\TAone{}$ do in the situation \ref{ex3b2}? Should he meet the raise, or fold?
\end{itemize}
Rest assured that answering these questions is not trivial at all! In fact, the answers will not come until Chapter \ref{chap:Nash}.





Maybe the first step in analyzing a game is to single out the answers to the following questions:
\begin{equation*}
\text{Relevant questions about a game: }
\left \{
\begin{aligned}
& \text{{1. Who are the players ?}} \\
& \text{{2. What can each player do ?}} \\
&  \text{{3. What are the possible states of the world ?}}\\
& \text{{4. What do they know of it?}}\\
& \text{{5. In what \emph{sequence} are the actions taken ?}}\\
& \text{{6. What do they get ?}}
\end{aligned} \right.
\end{equation*}
We invite you to answer these questions for the game of Example \ref{chap2:example:game}, and provide two examples next.

\begin{example}[Answers for the game of Chess]
$\,$\\
\begin{enumerate}
\item There are at least two players,
\item at their turn, they can move any chess piece and take one of the other player's pieces as long as it obeys the rules,
\item the configuration of the board (position of each pieces on it) defines what actions can be done, and their outcome,
\item they both see the board,
\item the white player starts, and then they take turn,
\item a player wins if e.g. he manages to check-mate his opponent, or his opponent forfeits, ...
\end{enumerate}
To the chess fans: we are leaving lots of interesting details aside.
\end{example}
\begin{example}[Answers for the game of Poker]
$\,$\\
\begin{enumerate}
\item There are two or more players around a table,
\item at their turn, players learn about their own hands (not the others'), they may bid, raise, fold, etc...,
\item there are a lot of possible hands that can be dealt,
\item they know their own hand, how much money has been bet by all players, who has folded, etc...,
\item players play in a clock-wise manner, taking turn with the first player being chosen at the begining of the game,
\item a player that has not folded and has the better hand wins all active bets.
\end{enumerate}
To the poker fans: we are leaving lots of interesting details aside.
\end{example}


In order to analyze such a game, we need first to represent it in a mathematical model upon which we will be able to perform a systematic analysis.



 Later, we will present the   \emph{strategic form}, which is a more \emph{concise} (bearing less information) model for games and focuses on the players,  their possible choices of actions, and the expected utility (e.g. monetary...) payoffs  these actions lead to.


\subsection{The extensive form}
\label{subsec:ExtForm}
The first model we present below is called the \emph{extensive form}.
The reason we start with this is that it is a very natural (and almost verbatim) representation of a game.  This has benefits and drawbacks, as it will contain lots of information,  providing us with a complete understanding of the structure of the game. However,  the amount of information  contained in the model
 often makes the analysis more cumbersome.


The extensive form describes a \emph{sequential (or dynamic) game}. Such a game is a sequence of \emph{events}.
Some events are due to chance, such as the random selection of the card color in Example \ref{chap2:example:game}.
Other events are controlled by players, and are more commonly referred to as \emph{moves}.
They correspond to a player doing something at some point of the game.
After a sequence of events, the game ends, and players receive their payoffs.

In order to capture these notions, the \emph{extensive form game} takes to form of a \emph{tree.}
 For the sake of understanding, we start by illustrating the concepts with Example \ref{ch2:ex:theTree}.


\begin{example}\label{ch2:ex:theTree}
The extensive form of the game of Example \ref{chap2:example:game}
is given in Figure \ref{chap2:example:figtree} as an illustration.
\begin{figure}[!ht]
\centering
\begin{tikzpicture}
\node[noeud-std] (src) {}
   [sibling distance=6cm]
   child {node[noeud-std] (n1a) {}
        [sibling distance=3cm]
         child[level distance=2cm]{node[noeud-std] (n2c1){}
		 [sibling distance=3cm]
         	child[level distance=1cm]{node[noeud-std,fill=white] (leaf1){} }
         	child[level distance=1cm]{node[noeud-std,fill=white] (leaf2){} }
         }
         child[level distance=1cm]{node[noeud-std,fill=white] (leaf3){} }
   }
	child {node[noeud-std] (n1b) {}
        [sibling distance=3cm]
         child[level distance=2cm]{node[noeud-std] (n2c2){}
		 [sibling distance=3cm]
         	child[level distance=1cm]{node[noeud-std,fill=white] (leaf4){} }
         	child[level distance=1cm]{node[noeud-std,fill=white] (leaf5){} }
         }
         child[level distance=1cm]{node[noeud-std,fill=white] (leaf6){} }
   }
   ;
\node[above=5pt] at (src) {$0$ };
\node[above=5pt] at (n1a) {$1.a$ };
\node[above=5pt] at (n1b) {$1.b$ };
\node[above=5pt] at (n2c1) {$2.c$ };
\node[above=5pt] at (n2c2) {$2.c$ };

\node[above=-20pt] at (leaf1) {$ 4, \, 0$ };
\node[above=-20pt] at (leaf2) {$ 3, \, 1$ };
\node[above=-20pt] at (leaf3) {$ 3, \,  1$ };
\node[above=-20pt] at (leaf4) {$ 0, \,  4$ };
\node[above=-20pt] at (leaf5) {$ 3, \, 1$ };
\node[above=-20pt] at (leaf6) {$ 1, \,  3$ };


\node[above left] at ($(src)!{0.25}!(n1a)$) {red: $0.5$};
\node[above right] at ($(src)!{0.25}!(n1b)$) {black: $0.5$};
\node[above right] at ($(n1a)!{0.15}!(leaf4)$) {Fold};
\node[above left] at ($(n1a)!{0.25}!(n2c1)$) {Raise};
\node[above left] at ($(n2c1)!{0.25}!(leaf1)$) {meet};
\node[above right] at ($(n2c1)!{0.25}!(leaf2)$) {pass};
\node[above right] at ($(n1b)!{0.25}!(leaf6)$) {fold};
\node[above left] at ($(n1b)!{0.25}!(n2c2)$) {raise};
\node[above left] at ($(n2c2)!{0.25}!(leaf4)$) {meet};
\node[above right] at ($(n2c2)!{0.25}!(leaf5)$) {pass};
\end{tikzpicture}

\caption{Extensive form of the game in Example \ref{chap2:example:game}}
\label{chap2:example:figtree}
\end{figure}

We construct it as follows. We begin by extracting information regarding the players, their actions, and their knowledge of the game.
\begin{itemize}
\item First, we let $N = \{1,2\}$ be the set of players for the game. Here, player 1 refers to $\TAone{}$, and player 2 refers to $\TAtwo{}$ in Example \ref{chap2:example:game}.
\item Then, we need to define for each player the possible \emph{information states.} Information states correspond to the different scenarios in which players have to make decisions.
We let $S_1 = \{a,b\}$ be the set of information states that player 1 may encounter, and $S_2 = \{c\}$ the set of the second player.\\
In the above, the state $a$ corresponds to the scenario where player 1 sees he has a red card, and $b$ to the case where he has a black card.
Note that player 2 only has a single information state: he only knows when it is his turn to play, he has no information about the state of player 1.
\textbf{It is critical, when modeling a game, to enumerate all relevant information states.}
\item At each information state $s$, we let $D_s$ be the set of actions that can be taken at this information state.
\end{itemize}
Next, we add structure to represent in which sequence the actions can be taken.
\begin{itemize}
\item We construct a tree on a set of nodes $V$. These represent three concepts: random events, decisions, or termination of the game, and we assume without loss of generality that all the payoffs are collected there. We thus represent these payoffs in the order corresponding to the players.
\begin{enumerate}
\item We let $V_0$ be the set of \emph{chance nodes}. In the tree of Figure \ref{chap2:example:figtree}, there is one chance node at the root of the tree with a label $"0"$. It corresponds to drawing a card, which can be either black or red with probability 1/2 each.
\item For each information state $s$, we let $Y_s$ be the set of \emph{decision nodes} for the state $s$. Note that, in the above, $Y_c$ has two nodes: we use this to represent the fact that when at state $c$, player 2 does not have perfect information about the state of the game.
\item Finally, we let $X$ be the set of \emph{leafs} in the tree. These correspond to the termination of the game.
\end{enumerate}
\item We then describe how we may navigate the tree.
\begin{enumerate}
\item To each chance node $v \in V_0$, we attach a distribution $p_x \in \Delta(V)$, which corresponds to the probability of reaching some node in the graph from the node $v$ as a result of uncertainty.
\item For each state $s \in \bigcup S_i$, for each node $y \in Y_s$ and each action $d \in D_s$ that can be chosen at this state, we let $t_s(y,d) \in V$ be the destination node attained in the tree.
\end{enumerate}
Once the tree is defined (of course, the distributions $p_v$ and transition functions $t_s$ much be chosen such that we obtain a tree), it actually provides a sequential representation of the game.
\item The last element we need is a \emph{utility function} $w : X \rightarrow \reels^N$, that assigns a value to each player in each leaf of the tree. Here, we simply put the amount of money each player has in the end of the game, assuming they each begin with 4 coins in their pockets.
\end{itemize}
\end{example}





\begin{notation}
If $f_1, f_2, \ldots, f_K$ are scalar functions, with $f_i : S \rightarrow \reels$,
then $f = (f_i)_{i \in \{1,\ldots, K\}}$ is a vector function $f : S \rightarrow \reels^K$.
For $s  \in S$,
$f(s) = (f_i(s))_{i \in \{1, \ldots, K\}}.$
\end{notation}

\begin{definition}[Games in extensive form]
\label{def:extform}
A game in extensive form is a structure $\Gamma_e = (N, S, D; V, V_0, Y, X; t, p; w)$,
where
\begin{itemize}
\item $N$ is a set of players\footnote{By a slight abuse of notations, we will use $N$ to denote either the set of players, or the number of players in a game. We will clarify if needed.}.
\item $S = \bigcup_{i \in N} S_i$ is a set of \emph{information states}, where $S_i$ is the set of information state for player $i$, and $S_i \bigcap S_j = \emptyset$ if $i  \neq j$.
\item $D = \bigcup_{s \in S} D_s$ is the set of \emph{moves}, where $D_s$ is the set of moves available for a  player in information state $s \in S$.
\item $V$ is a set of \emph{nodes}.
\item $V_0 \subset V$ is the set of \emph{chance nodes}.
\item $Y = \bigcup_{s \in S} Y_s \subset V$ is the set of \emph{action nodes}, where $Y_s$ is the set of nodes with information state $s$.
\item $X \subset V$ is the set of \emph{leafs}.
\item $t = (t_s)_{s \in S}$ are transitions, where for each information state $s$,  $t_s : Y_s \times D_s \rightarrow V$ assigns to every \emph{source node} $y \in Y_s$ and action $d \in D_s$ a destination node $x \in V$.
\item $p = (p_{x})_{x \in V_0}$ assigns a probability distribution $p_x \in \Delta(V)$ to each chance node $x \in V_0$.
\item $w : X \rightarrow \reels^N$ is a utility function, assigning to each leaf $x \in X$ the payoff of every players if the game ends up at that leaf.
\end{itemize}
\end{definition}

Again, when presenting a game in extensive form, we prefer to rely on their graphical representation. Here are some conventions for their drawing:


\begin{notation}[Conventions for drawing an extensive form game]
In order to \emph{represent} the tree, we follow a classical set of conventions to label nodes and edges,  as in Figure \ref{chap2:example:figtree}.
\begin{enumerate}
\item We put a label $0$ on a chance node.
\item Under each leaf, we write a vector with one entry per player to denote the utilities of the players at this node.
\item We label each decision node with a pair of the form (player playing at the node, information state of the node).
\item We label each edge outgoing from a chance node with the probability of this transition being taken.
\item We label each edge outgoing from a node $Y_s$ with its corresponding action in $D_s$.
\end{enumerate}
\end{notation}



\begin{exercise}
Propose an extensive form model for the following games.
\begin{itemize}
\item Heads-or-tails, with 2 players each betting a dollar.
\item Rock-Paper-Scissors., with 2 players each betting a dollar.
\item The game of Example \ref{chap2:example:game} where now, \TAtwo{} picks a card from a deck of 53 cards, with 26 red cards, 26 black cards, and one joker. If he gets a joker \TAone{} immediately wins the bet, and the game ends.
\item The game of Example \ref{chap2:example:game}, when played twice in a row.
\end{itemize}
\end{exercise}

% Comparison with lotteries and such ...
% Recall def of lottery
% State intuitively for each player what can be the "state of the world"
% Recall that by the theorem, there is a probability distribution on state of the world,
% which means that player $i$ will hold beliefs (in terms of probabilities) about what actions the other players are going to do.

\subsubsection{Decision theoretic perspective}

We make a short digression to discuss the concepts above from the perspective of Decision theory, through the concepts introduced in Chapter \ref{chap:Decision}.

As a reminder, in decision theory, a player needs to make a decision which leads him to obtaining a prize. This is formalized by the concept of \emph{lottery} (see Definition \ref{ch1:def1:lottery}).
A lottery is a function
$$f : \Omega \rightarrow \Delta(X), $$
where $\Omega$ is the set of possible states of the world, and $X$ are the possible prizes (outcomes) the player may obtain. In this section, we take the time to equate the notions of states of the world and prizes with their parallels in the extensive form.

Let us begin with the concept of decision. In Chapter \ref{chap:Decision}, we associated to each decision of the player a lottery.
 In our current context, the decision of a player $i$ consists in choosing, at all $s \in S_i$, an action $d \in D_s$ to perform.\\
 This leads us to the concept of \emph{pure strategy}. \\
 \begin{definition} [Pure strategy]
Given a game in extensive form $\Gamma_e$, a \emph{pure strategy} for a player $i \in N$ is an element
$c \in C_i = \times_{s \in S_i} D_s$, acting as a function that associates to each information  state of the player a \emph{move} to be played at that state.
\end{definition}
\begin{example}
In Example \ref{chap2:example:game},
\TAtwo{} has 4 pure strategies. Indeed, he has 2 information states,
with two moves at each. A possible strategy is
\emph{play Raise on a red card, play fold on a Black card}.\\*
\TAone{} has 2 pure strategies, since he has only 1 information state and 2 available moves at this state.
These strategies are \emph{always raise} and \emph{always pass}.
\end{example}
 Later, and in particular in Chapter \ref{chap:Seq}, we consider the possibility for a player to chose a randomized strategy: he will pick at each state $s \in S_i$ a distribution $\sigma_s \in \Delta(D_s)$, and pick $d \in D_s$ with probability $\sigma_s(d)$.

Once such a decision in made, the player receives a prize depending on the realization of uncertainty at chance nodes (which represent objective probabilities) as well as the moves of other players (which are subjective in the sense that they are not defined as part of the game). Hence, the \emph{state of the world} for a player making a decision at state $Y_s,$ denoted $\Omega_s,$ will be an element of the set containing all the subsequent decisions played by other players at the later information states.

Interestingly, this alone teaches us something about how a rational and intelligent decision maker should reflect on a game! Indeed, the Utility Maximization Theorem states that a player should possess a \emph{conditional probability function}, expressing thus his own beliefs on what the other players will do in the game. We will develop that idea in full extent in Chapter \ref{chap:Seq}.
\subsection{The strategic form}
\label{subsec:StratForm}

A \emph{strategic form game} can be seen as a summary of an extensive form game, where we keep only the answers to the questions
\begin{center}
\textit{Who are the players, what can they do, and what they will receive by doing so?}
\end{center}

To answer this question, we focus on \emph{pure strategies} and they \emph{payoffs}.


\begin{definition}[The strategic form]
A \emph{strategic form game} is a tuple
$$ \Gamma = (N, C, u),$$
where
\begin{itemize}
\item $N$ is the non-empty set of players in the game,
\item $C = \bigtimes_{i \in N}C_i$ is the set of all \emph{pure strategies} for all players,
\item and $u = (u_i)_{i \in N} : C \rightarrow \reels^N$ is the payoff function.
The payoff $u_i : C \rightarrow \reels^N$ of player $i$
is the expected payoff of the player.
\end{itemize}
When constructing a strategic form game from a game in extensive form, the payoffs are computed as follows:
$$u_i(c) = \sum_{\text{leaf $\ell$ in the tree}} p(\ell|c) w_i(\ell), $$
where $w_i(\ell)$ is the payoff of player $i$ at a leaf denoted by $\ell$ in the tree, and $p(\ell|c)$ is the probability of reaching that leaf if the players picked their strategies following $c$.
\end{definition}
Throughout the course, we will focus on finite games, that are games where both the sets $N$ and $C$ are finite.
\begin{example}
For Example \ref{chap2:example:game}, the set of players is $N = \{1,2\}$,
the pure strategies for each players are
 $$C_1 = \begin{pmatrix}
Raise/raise, \\
Raise/fold, \\
Fold/raise, \\
Fold/fold.
\end{pmatrix}, C_2 = \begin{pmatrix}
meet,\\
pass.
\end{pmatrix},$$
and the set $C$ is the cartesian product of $C_1$ and $C_2$.
Consider now the strategy profile $c = (Raise/fold, meet)$.
The payoffs are then given by
$$u(c) = \frac{1}{2} \cdot (4,0) + 0 \cdot (3,1)+ 0 \cdot (0,4) + 0 \cdot (3,1) + \frac{1}{2} \cdot (1,3) = (2.5,1.5).$$
\end{example}

We now introduce the
 \emph{normal representation of a game in strategic form},
a compact way to present a strategic form game.
A normal representation is a table in $N$ dimensions
 (one dimension per player),
 and represents the payoff function
 $u : C \rightarrow \reels^N$.
 We prefer an example to a definition here:
\begin{example}
The \emph{normal representation} of the game of Example $\ref{chap2:example:game}$ is given at Figure \ref{chap2:table}.
By convention, we always refer to the player on the left of the table as the \emph{first} player,
which means that the first entry in the payoff vector is always his payoff.
\begin{figure}[!ht]
\centering
\begin{tabular}{l|cc}
\TAtwo{} vs \TAone{} & meet & pass \\
\hline
Raise/raise & 2, 2 & 3, 1 \\
Raise/fold & 2.5, 1.5 & 2, 2 \\
Fold/raise & 1.5, 2.5 & 3, 1 \\
Fold/fold & 2, 2 & 2, 2
\end{tabular}
\caption{Normal representation of Example \ref{chap2:example:game}}
\label{chap2:table}
\end{figure}

\end{example}

The strategic form is built using \emph{pure} strategies. In practice, there are games where one would rather \emph{randomize} between strategies.
For example,  in rock-paper-scissor, the pure strategies are to play \emph{rock},  \emph{paper} or \emph{scissors}. However, in practice, people often try to play rock, paper or scissor with probability 1/3 each.

\begin{definition}[Randomized strategy]
Given a player $i \in N$ and its set of strategies $C_i$, a \emph{randomized strategy} for the player is a probability distribution $\sigma_i \in \Delta(C_i)$:
$$ \forall c_i \in C_i: \sigma_i(c_i) \geq 0; \qquad \sum_{c_i \in C_i} \sigma_i(c_i) = 1.$$
A randomized strategy profile $\sigma \in \Delta(C)$ is a set of one randomized strategy per player.
The \emph{expected payoff} of player $i \in N$ for the profile $\sigma \in \Delta(C)$ is given by
$$E_\sigma(u_i(c)) = \sum_{c \in C} \sigma(c) u_i(c)= \sum_{c_i \in C_i} \sigma_i(c_i) \sum_{c_{-i} \in C_{-i}} \sigma_{-i}(c_{-i}) u_i(c_i, c_{-i}), $$
where ``$-i$'' denotes the set of all players except $i$, and
$\sigma_{-i}(c_{-i}) = \prod_{j \neq i}\sigma_{j}(c_j)$, with $c_{-i} = (c_j)_{j \neq i}$.
\end{definition}


Given a game in extensive form, there is a unique strategic form representation for the game. The reverse is not true. In particular, if we are given a game in strategic form, we cannot retrieve the order in which the players will play the game.\\
This sequentiality is sometimes necessary to define the behavior of intelligent and rational agents (or, oppositely, to rule out some behaviours as being irrational).  We will see that in Chapter \ref{chap:Seq}. \\
However, it is often sufficient to study the normal representation of a game for analysis.

\section{Best responses and domination}

In the previous section, we \emph{represented games}. We did not approach the following fundamental question:
 \textbf{What should (or will?) each player do in a game?}

This question is similar to the one targeted by \emph{Decision Theory} (Chapter \ref{chap:Decision}).
For game theory, we ask ourselves what should be the decision taken by a single player if he wants to maximize his own gains, \emph{and he knows that the other players too.}



We will first introduce the concept of \emph{best response strategy}. It is a very important concept for understanding the behavior of rational players, because this simple concept, derived from our axioms of decision theory, will lead as to a (partial) solution to the above question. The rationale is the following: if a particular strategy can never be optimal, whatever the other players do, then, one can for sure rule it out of its sensible actions.



\subsection{Best responses}
\label{chap2:subsec:BR}

\begin{definition}[Best response]
Consider a player $i \in N$. If the players $-i$ play following the strategy profile $\sigma_{-i} \in \Delta(C_{-i})$, then the set of \emph{best response strategies} for player $i \in N$ is given by
$$ \argmax_{c_i \in C_i} \sum_{c_{-i} \in C_{-i}} \sigma_{-i}(c_{-i}) u_i(c_i, c_{-i}).$$
\end{definition}

The fact that best responses are a central concept in game theory can be seen as a direct consequence of decision theory. If player $i$ associates a probability $\sigma_{-i}(c_{-i}) $ that other players play the strategy $c_{-i}$, then picking a best-response strategy corresponds to maximizing its expected utility (Theorem \ref{ch1:utility-maximization}), which is the expected behavior of an intelligent and rational player.

Now, one can push this reasoning further: Since we can assume that every player in the game wants to pick a best response, we can iterate this argument, allowing us to refine our prediction (or recommendation) on the decision of the players.
Indeed, the choice of a best response for player $i$ is driven by what he thinks others will play: $\sigma_{-i}(c_{-i}) $. However, since the other players want to pick best responses as well, player $i$ might be able to adapt his beliefs $\sigma_{-i}(c_{-i}) $.

This observation leads to a recursive way to  find the \emph{best} strategy to play. First, pick an initial strategy $\sigma_{-i}$,  then repeat
\begin{itemize}
 \item Get $c_i$ as a best response to $\sigma_{-i}$,
 \item Update $\sigma_{-i}$ so that it corresponds to a best response from the other players to $c_i$.
\end{itemize}

Sadly, things are not that easy. It is straightforward to build examples where this approach does not converge...

\begin{example}
Take again the game of example \ref{chap2:example:game}. \TAtwo{} wants to find the best strategy. We advise you to follow the reasoning by referring to the normal representation of the game at Figure \ref{chap2:table}.

\begin{itemize}
\item Assume first \TAone{} is going to ``pass'' everytime. Then \TAtwo{} should play ``Raise/raise'' or ``Fold/raise'', which guarantees a payoff of $3\$$.
\item If \TAtwo{}s plays as defined above, \TAone{} should play ``meet'', which gives him an expected payoff of $2.25\$$.
\item In that case \TAtwo{} should play ``Raise/fold''!...
\item ...to which the best response from \TAone{} is ``pass'', which brings us back at the beginning of the analysis.
\end{itemize}
\label{chap2:example:bestresponseequilibria}
\end{example}

We have to be more clever...

\subsection{Domination}

A rational player always plays what he believes is a best response to the other moves. Knowing this, we can sometimes rule out the possibility that one player plays a particular strategy $c_i$, in particular when we can show that $c_i$ can never be a best response.

 \begin{definition}[Strong domination]
 For a player $i \in N$, a strategy $d$ is \emph{strongly} dominated if it is \emph{never} a best response:
 $$\forall \sigma_{-i} \in \Delta(C_{-i}), \  d \not \in \argmax_{c_i \in C_i} \sum_{c_{-i} \in C_{-i}} \sigma_{-i}(c_{-i}) u_i(c_i, c_{-i}).$$
 \label{chap2:defdomibr}
 \end{definition}

\begin{example}
\label{chap2:exampleintrodomi}
Corentin and Fran\c{c}ois play the game in normal representation at Figure \ref{chap2:videogame}.
Have a closer look to strategies $B$ and $D$, and observe that for any pure strategy of Corentin, \emph{Fran\c{c}ois always has a greater payoff by playing $D$ instead of $B$}.
Fran\c{c}ois being an intelligent and rational player, he will never play $B$, which cannot be a best response and is strongly dominated.
We could even erase $B$ from the game!
 \begin{figure}[!h]
\centering
\begin{tabular}{c|cccc}
  Corentin vs Fran\c{c}ois &  A &  B & C & D\\
\hline a & 8, 5 &  4, 6 & 1, 6 & 7, 8 \\
 b & 5, 6 &  7, 7 & 4, 4 & 2, 9 \\
 c & 6, 7 &  5, 5 & 2, 6 & 3, 6 \\
 d & 5, 4 &  5, 2 & 6, 5 & 7, 5 \\
\end{tabular}
\caption{A two player game with 4 pure strategies each. Corentin plays a,b,c,d; Fran\c{c}ois plays A,B,C,D.}
\label{chap2:videogame}
\end{figure}

\end{example}

There is an equivalent definition of strong domination, which is often easier to check. With the following, we also define the idea of \emph{weak domination}: in some cases, a player might play a weakly dominated strategy, but there is always another strategy at least as good than the first one, better in some cases.

\begin{definition}[Weak and strong domination]
A pure strategy $d_i$ is \emph{weakly dominated} if there exists a randomized strategy $\sigma_i \in \Delta(C_i)$ such that, $\forall c_{-i} \in C_{-i}$:
\begin{equation}
 u_i(d_i, c_{-i}) \leq \sum_{e_i \in C_i} \sigma_i(e_i) u_i(e_i, c_{-i}),
 \label{domination}
\end{equation}
with the inequality being strict for at least one\footnote{naturally, if the inequality is never strict, then the strategies are equivalent.} strategy in $c_{-i}$.\\*
The strategy $d_i$ is \emph{strongly dominated} if the inequality (\ref{domination}) is strict for all $c_{-i}$.
\label{defdomipayoffs}
 \end{definition}


\section{Equivalent games}
\label{chap2:subsec:Equivalences}

In this section, we characterize different notions of \emph{equivalence} between games.  Equivalence here relates to the similar notion present in Decision Theory (see Chapter \ref{chap:Decision}, Theorem \ref{chap1:thm:Equivalent}). Intuitively, we will seek for a notion of equivalence that encompasses the fact that players behave the same in both games. So, two games might have very different available payoffs but yet be equivalent, if these payoffs can never occur in practice (i.e. in situations where players are rational.)

\begin{definition}[Full equivalence]
The two games $\Gamma = (N, C, u)$ and $\hat \Gamma = (N,C,\hat{u})$ are \emph{fully equivalent} if
$$\,\,\forall i \in N, \exists A_i > 0, B_i : \hat u_i(c) = A_i u_i(c) + B_i, \forall c \in C.$$
\label{chap2:def:fullEquivalence}
\end{definition}

\begin{example}
Adeline and Emilie want to model the rock-paper-scissor game with a normal representation. \\*
Adeline's version is as follows:
\begin{center}
\begin{tabular}{c|cccc}
   & R &  P & S \\
\hline r & 0, 0 &  -1, 1 & 1, -1 \\
 p & 1, -1 &  0, 0 & -1, 1 \\
 s & -1, 1 &  1, -1 & 0, 0 \\
\end{tabular} ,
\end{center}
Emilie's version is as follows:
\begin{center}
\begin{tabular}{c|cccc}
   & R &  P & S \\
\hline r & 1, 1 &  -1, 3 & 3, -1 \\
 p & 3, -1 &  1, 1 & -1, 3 \\
 s & -1, 3 &  3, -1 & 1, 1 \\
\end{tabular} .
\end{center}
Let us now reflect on two questions:
\begin{itemize}
\item Which one would you rather play?
\item Would you behave differently in playing  one game or the other?
\end{itemize}
The first one is a tricky question and id depends on the utility scale in each game.\\
The second question however, admits a clear answer: a rational and intelligent decision maker will make the same decisions in both games, that is,
the games are fully equivalent. Indeed, he payoff in the second game are obtained by multiplying the ones in the first by 2, and then adding 1.
\end{example}

Full equivalence is a property that is difficult to satisfy, and is mostly used in proofs.
 The next definition allows for an even more succint representation of the game, with a focus on important strategies.
\begin{definition}[Best-response equivalence]
Two games $\Gamma=(N, C, u)$ and $\hat \Gamma = (N,C, \hat u)$  are  \textit{best-response} equivalent if and only if
they have the same best response sets.
That is,
$\forall i \in N$, $\forall \sigma_{-i} \in \Delta C_{-i}:$\\
$$ c'_i \in \arg \max_{c_i \in C_i} \sum_{c_{-i} \in C_{-i}}
\sigma_{-i}(c_{-i}) u_i(c_i, c_{-i}) \Leftrightarrow c'_i \in \arg \max_{c_i \in C_i} \sum_{c_{-i} \in C_{-i}}
\sigma_{-i}(c_{-i}) \hat{u}_i(c_i, c_{-i}).
$$
\end{definition}

Full equivalence implies best-response equivalence.
Remember that decision makers will always pick what they believe is a best response. Once they have fixed an assumption on the actions of the other players, they play a best responses to maximize expected payoff.
When two games are best-response equivalent, the decision making is therefore preserved.


\begin{example}
Vladimir and Nikos play the two following games:

\begin{center}
$\Gamma = \Bigg \{ $
\begin{tabular}{l|cc}
 & $x_2$ & $y_2$  \\
\hline
$x_1$ & 9, 9 & 0, 8 \\
$y_1$ & 8, 0 & 7, 7
\end{tabular} $\Bigg \}, \qquad \hat{\Gamma} =  \Bigg \{  $
\begin{tabular}{l|cc}
  & $x_2$ & $y_2$  \\
\hline
$x_1$ & 1, 1 & 0, 0 \\
$y_1$ & 0, 0 & 7, 7
\end{tabular}
$\Bigg \},$
\end{center}
and we wonder whether they will behave differently in one game or another. We will show that $\Gamma$ and $\hat \Gamma$ are best response equivalent, and therefore the players should behave the same in both game. To do so, we exploit the symmetry of the game.\\*
We put ourselves in the shoes of player 1, and assume that player 2 plays $x_2$ with probability $\alpha$ and $y_2$ with probability $1-\alpha$.\\*
In the first game, the expected payoffs are
$$u_1(x_1, \alpha x_2 + (1-\alpha) y_2) = 9 \alpha, \qquad u_1(y_1, \alpha x_2 + (1-\alpha) y_2) =  \alpha + 7. $$
Therefore, we play $x_1$ if and only if $9\alpha \geq \alpha + 7$, or $\alpha \geq 7/8$ (plain lines in Figure \ref{chap2:breq}).\\*
In the second game, we have
$$\hat{u}_1(x_1, \alpha x_2 + (1-\alpha) y_2) =  \alpha, \qquad \hat{u}_1(y_1, \alpha x_2 + (1-\alpha) y_2) =   7 - 7\alpha. $$
Again, we will play $x_1$ if and only if $\alpha \geq 7/8$ (dotted lines in Figure \ref{chap2:breq}),
and we can conclude best response equivalence.
\begin{figure}[!ht]
\centering
\includegraphics[width=0.5 \textwidth]{breq.eps}
\caption{Best responses illustrated.}
\label{chap2:breq}
\end{figure}
\end{example}


\subsection{Reducing a game}
\label{chap2:sec:reduction}

As always when studying a problem, the complexity of the analysis grows as the size of the problem grows.
In this subsection, we give techniques that allow to reduce the size of a game by removing strategies.\\

\noindent\textbf{\underline{Elimination of redundant strategies.}}

\begin{definition}[Redundant strategies]
We say that the pure strategy $c_i \in C_i$ is \emph{randomly redundant} or \emph{payoff equivalent} if there is a random strategy $\sigma_i \in \Delta(C_i \setminus c_i)$ such that $\forall c_{-i} \in C_{-i}$ and for all players $j \in N$
$$u_j(c_i, c_{-i}) = \sum_{e_i \in C_i} \sigma_i(e_i) u_j(e_i, c_{-i}).$$
\end{definition}

For a player $i$ who has a redundant strategy, this strategy can be removed and replaced by a randomized one with the same effect. \\*
Take another player now, and say he believes the player $i$ will play $c_i$ with probability $\alpha$. Then, the best response set to that situation is the same to the one where player $i$ plays the equivalent strategy with probability $\alpha$, and $c_i$ with probability 0.\\*
If a randomly redundant strategy is found, then we can remove it from the game without changing the set of best responses (except, maybe, by removing from it redundant strategies).
The normal representation obtained after removing all redundant strategies from a game is called the \emph{fully reduced normal representation}. \\

\noindent\textbf{\underline{Elimination of dominated strategies.}} \\

In order to further reduce a game, we may try to construct a \emph{best response equivalent game} which would be \emph{as small as possible}. A natural approach is to remove all the strategies that can never be best-responses, which are, by definition, \emph{strongly dominated strategies}.\\
The following recursive procedure allows to eliminate all \emph{strongly dominated} strategies from a game.

\begin{procedure}[Elimination of dominated strategies]
Consider a strategic form game $\Gamma$. The following procedure converges:
\begin{enumerate}
\item Let $\Gamma^0 = \Gamma $ be a game in strategic form, an let $k = 0$.
\item Let $\Gamma^{k + 1}$ be the game formed by removing all (or at least some) strongly dominated strategies from $\Gamma^k$. \label{domiiter}
\item If $\Gamma^{k+1} = \Gamma^k$, stop, and let $\Gamma^\infty = \Gamma^k$. Else, do $k := k+1$, and go back to step \ref{domiiter}.
\end{enumerate}
\label{chap2:domioutproc}
\end{procedure}

It is easy to show that any agent will always play a strategy of the game $\Gamma^{\infty}$. Indeed, for any $k \geq 0$, a player should always play a strategy from the game $k+1$, to avoid strong domination.\\*
The above procedure is certain to converge since we consider finite games. There can only be a finite number of strategies to remove. \\*
A less obvious fact is that the game $\Gamma^\infty$ is unique. In fact, we can consider another procedure where, at step \ref{domiiter}, only one strongly dominated strategy is picked and removed. We would still converge to the same $\Gamma^\infty$.\\* If we try to eliminate \emph{weakly} dominated strategies, then the solution is no longer unique, as illustrated in the next example.

\begin{example}
Consider the game of Example \ref{chap2:exampleintrodomi}.
The game $\Gamma^0$ is the one represented in Figure \ref{chap2:videogame}.
We already saw that strategy $B$ was dominated by strategy $D$, so $\Gamma^1$ does not contain $B$. Removing this strategy, we obtain:
\begin{center}
$\Gamma^1 = \Bigg \{$
\begin{tabular}{c|ccc}
  Corentin vs Fran\c{c}ois &  A   & C & D\\
\hline a & 8, 5  & 1, 6 & 7, 8 \\
 b & 5, 6  & 4, 4 & 2, 9 \\
 c & 6, 7  & 2, 6 & 3, 6 \\
 d & 5, 4  & 6, 5 & 7, 5 \\
\end{tabular}
$\Bigg \}.$
\end{center}

Inspecting $\Gamma^1$, we see that if Corentin was to randomize between $a$ and $d$, he might get higher payoff than with strategy $c$.
Let us verify this: if he plays $a$ with probability $\alpha$ and $d$ with probability $(1-\alpha)$ then $c$ is strongly dominated if
$$ 8\alpha + 5(1-\alpha) > 6; \, \alpha + 6(1-\alpha) > 2; $$
which holds true for $ 1/3 < \alpha < 4/5$.\\*
Moreover, strategy $b$ is also strongly dominated by $\beta a + (1-\beta) d$ for all $ 0 < \beta < 2 /5$.\\*
We therefore obtain
\begin{center}
$\Gamma^2 = \Bigg \{$
\begin{tabular}{c|ccc}
  Corentin vs Fran\c{c}ois &  A   & C & D\\
\hline a & 8, 5  & 1, 6 & 7, 8 \\
 d & 5, 4  & 6, 5 & 7, 5 \\
\end{tabular}
$\Bigg \}.$
\end{center}

We can now see that $A$ is strongly dominated by $D$, and obtain the game
\begin{center}
$\Gamma^{\infty}  = \Gamma^3 = \Bigg \{$
\begin{tabular}{c|cc}
  Corentin vs Fran\c{c}ois   & C & D\\
\hline a  & 1, 6 & 7, 8 \\
 d   & 6, 5 & 7, 5 \\
\end{tabular}
$\Bigg \}.$
\end{center}

Remark that 1) by inspecting $\Gamma^1$ only, we cannot see the domination of $A$ by $D$, which justify the recursive elimination procedure (Procedure \ref{chap2:domioutproc}).

Observe now that there is some \emph{weak} domination in $\Gamma^\infty$. However, if we wanted to eliminate those strategies, we would not obtain an unique solution.
For example, removing $a$ in favor of $d$, we have
\begin{center}
$\Gamma^{\infty}\backslash \{ a \}= \Bigg \{$
\begin{tabular}{c|cc}
  Corentin vs Fran\c{c}ois   & C & D\\
\hline d   & 6, 5 & 7, 5 \\
\end{tabular}
$\Bigg \},$\\*
\end{center}
but we could also remove $C$ in favor of $D$:
\begin{center}
$\Gamma^{\infty}  \backslash \{C\} = \Bigg \{$
\begin{tabular}{c|c}
  Corentin vs Fran\c{c}ois   & D\\
\hline a   & 7, 8 \\
 d   & 7, 5 \\
\end{tabular}
$\Bigg \}.$
\end{center}
In any case, once we have removed one, we cannot remove the other.
\end{example}

\section{Common knowledge and private information}


In many games, players have access to private information.
Examples include the color of the card \TAtwo{} sees in example \ref{chap2:example:game}, a poker hand, the maximum amount of money one is willing to put on an auction... When we think of it, in many games involving some kind of ``bluff'', we often try to convince other players that our private information is the most advantageous to us.
In example \ref{chap2:example:game}, \TAone{} would rather play ``meet'' if he thinks \TAtwo{} has a black card, and ``pass'' if he has a red card. Therefore, \TAtwo{} would prefer \TAone{} to believe he has a red card when he in fact has a black card, and vice-versa...

This being said, we understand how this concept of private information comes to play a very important role in the analysis of games. Let us first define formally
what private information, and its opposite \emph{common knowledge} means.

\begin{definition}[Common knowledge]
A \emph{private information} is any knowledge on the state of the world that is not a \emph{common knowledge.} An information $X$ is \emph{common knowledge} if the statement
\begin{center}
\emph{$($everybody knows that$)^K$ everybody knows X}
\end{center}
holds for all $K = 1, 2, \ldots$.
\end{definition}

 The somewhat recursive definition of common knowledge seems to be complicated.
 It may appear that ``I know X'' and ``You know X'' should be enough so that $X$ is common knowledge. The following well known example shows otherwise.

 \begin{example}
 We present here the famous \emph{paradox of the generals}.
 Two generals, Rapha\"el and Julien plan on capturing an enemy fort in a valley.
 Their respective encampments are located at opposite sides of the valley.
 The only viable tactic is a coordinated attack: they must attack on both sides of the fort at the same time. Else, the fort defences will organize and easily defeat them. \\*
 The generals need to agree on the hour of the attack. To do so, they send messengers through the valley.
\begin{itemize}
\item Assume first that there is a 100 \% chance that a messenger sent by one side arrives at the other side. Then, if Rapha\"el sends a messenger, he knows Julien will receive the message. If Rapha\"el says \emph{I'm attacking before lunch}, then Julien will necessarly follow since not doing so implies a defeat.
\item Assume now that both Julien and Rapha\"el \emph{believe}\footnote{It does not need to be a fact!} that there is a non zero probability that the messenger is intercepted during the carrying of the message.
\begin{enumerate}
\item When Rapha\"el sends a message, he cannot be certain Julien receives it. \item Thus, Julien should confirm the reception of the message to Rapha\"el, by sending a messenger back.
\item If Rapha\"el receives this message, then he knows that Julien knows the hour of the attack. But Julien will not move unless he is certain that Rapha\"el knows that Julien knows the hour of the attack. Thus, Rapha\"el sends another messenger to Julien.
\item Now, Julien received this messenger, and knows that Rapha\"el knowns that Julien knows the hour of the attack. But...
\end{enumerate}
At the end, what the story says is that if both generals want to be certain to win the battle, they will have to send an infinite number of messengers.
This is because they want the hour of the attack to be a \emph{common knowledge.}
\end{itemize}
\end{example}

\subsection{Modelling games with incomplete information}
\label{ch2:bayesianGames}

A game with incomplete information is a game in which some player possesses private information on the game before it even begins. To represent these  informations, we assign to the players a \emph{type} that identifies the information. For example, players in an auction know before the auction the amount of money they are willing to spend. Then, we could say that a player is of type ``I'll spend at most 1000 \$'', etc...

 In what follows, we assume that there is a finite number of possible types for each player. The set of types for player $i$ is written $T_i$.

A natural way to model a game is to assume that the types will be assigned before the beginning of a game with all the appropriate randomness. This leads us to an extensive form representation of the game, using a chance node at the root of the tree.
\begin{example}
Consider example \ref{chap2:example:game} with a twist. \TAone{} has a dangerous gambling addiction, and sees \TAtwo{} with a card in his pocket.  Without hesitation, he proposes to \TAtwo{} a bet on the color of the card, where \TAtwo{}, as in example \ref{chap2:example:game}, can raise the bet if he wishes so.\\
A way to model this game is to use the same extensive form to that of Figure \ref{chap2:example:figtree}. The difference is that the first chance node, instead of representing the drawing of the card, represents the realization of the \emph{type} of \TAtwo{}, that is the color of the card in his pocket.
\label{ch2:exbayintro}
\end{example}

Modeling a game with private information using an extensive-form-like structure has some drawbacks.
A first practical drawback concerns the size of the tree. For an N player game, the set of all type-profiles $T = (T_i)_{i \in N}$ can be quite big - and we need as many branches per type profile in $T$ to represent the game. \\*
A second drawback appears when considering the temporal aspects of the game.
An extensive form presents a clear, sequential, step-by-step game, where the root to the tree is the beginning of the game. When representing the possible types in extensive form, this root corresponds to a moment \emph{before} the beginning of the game...

In order to model games with incomplete information, a generalisation of the \emph{strategic form}, called a \emph{bayesian game} is often preferred to the extensive form.

\begin{definition}[Bayesian game]
A \emph{bayesian game} is a tuple $$\Gamma^b = (N,C,T,p,u), $$
where
\begin{itemize}
\item N is the non-empty set of players,
\item $C = (C_i)_{i \in N}$ is the set of all pure actions for all players,
\item $T = (T_i)_{i \in N}$ is the set of all player types;
\item $p = (p_i)_{i \in N}$ are \emph{beliefs functions},
$$ p_i : T_i \rightarrow \Delta(T_{-i}),$$
\item and $u = (u_i)_{i \in N}$ are the payoff functions,
$$ u_i : C \times T \rightarrow \reels. $$
\end{itemize}
The game is finite if $N$, $C$ and $T$ are finite.
\end{definition}
 \begin{example}
 Back to example \ref{ch2:exbayintro}.

 \TAtwo{} has two types: $T_1 = \{\text{red}, \text{black}\}$. \TAone{} has one type $T_2  =\{\text{gambler}\}$. Therefore, the set $T$ contains only two entries.
 The payoff function can be represented by these two tables:
\begin{center}
\begin{tabular}{l|cc}
red & meet & pass \\
\hline
Raise & 4/0 & 3/1 \\
Fold  & 3/1 & 3/1 \\
\end{tabular}, $\qquad$
\begin{tabular}{l|cc}
black & meet & pass \\
\hline
Raise & 0/4 & 3/1 \\
Fold  & 1/3 & 1/3 \\
\end{tabular}.
\end{center}


 The belief function of \TAtwo{} is $p_1(\text{gambler}|\text{red}) = p_1(\text{gambler}|\text{black}) = 1.$
The belief function of \TAone{} is $p_2(\text{red} | \text{gambler}) = \alpha$,  $p_2(\text{black}|\text{gambler}) = 1-\alpha$.
 \end{example}

 Representing the belief functions of each player can be cumbersome. This can be made easier is the beliefs are \emph{consistent}:
 \begin{definition}[Consistent beliefs]
 We say that the belief functions $p = (p_i)_{i \in N}$ are \emph{consistent} if there exists an a-priori probability distribution $P \in \Delta(T)$ such that for all $t_{-i} \in T_{-i} $, $t_i \in T_i$,
  $$ p_i(t_{-{i}} | t_i) = \frac{P(t_i, t_{-i})}{\sum_{s \in T_{-i}} P( t_i, s) } $$
 \end{definition}
It turns out, we can show that any bayesian game is \emph{equivalent} to a bayesian game with consistent beliefs.


In fact, bayesian games are not essentially different from general standard games.  To see this, one can see that, given any bayesian game,  one can build an equivalent standard game, named the 'type-agent representation' of the bayesian game.

 \begin{definition}[Type-agent representation]
 Given a Bayesian game $$\Gamma^b = (N,C,T,p,u), $$
 the type-agent representation the following strategic form game:
 \begin{itemize}
 \item The set of agents is $T_1 + T_2 + \ldots + T_N$.
 We write $t_k \in T_i$ to denote the fact that \emph{agent k} belongs to player $i$.
 \item If $t_k \in T_i$, then the pure strategies for $t_k$ are $C_i$.
 \item The payoff function of agent $t_k \in T_i$ is given by
 $$v_{t_k}(c_{t_1}, \ldots, c_{t_T}) = \sum_{t_{-i}}p_i(t_{-i}|t_i) u_i(c, (t_k, t_{-i})). $$
 \end{itemize}
 \end{definition}
  \begin{example}
  The following table shows the payoffs for the type-agent representation of the game of example \ref{ch2:exbayintro}. We assume that the probabilities that \TAtwo{} has a red or black card are the same.
  These payoffs are to be read
  $$ (\text{\TAone{}'s payoff}/\text{red \TAtwo{}'s payoff}/\text{black \TAtwo{}'s payoff}).$$
  \begin{center}
 \begin{tabular}{l|cc}
 & red:Raise & red:Fold \\
\begin{tabular}{l}
\\
\hline meet \\
pass
\end{tabular} &
\begin{tabular}{cc}
\hline black:raise & black:fold \\
\hline 2, 4, 0 & 1.5, 4, 1 \\
1, 3, 3 & 2, 3, 1
\end{tabular}&
\begin{tabular}{cc}
\hline black:raise & black:fold \\
\hline 2.5, 3, 0 & 2, 3, 1 \\
1, 3, 3 & 2, 3, 1
\end{tabular}
\end{tabular}
\end{center}

 \end{example}
\ifx \globalmark \undefined %% This is default.
\bibliographystyle{plain}
\bibliography{../gametheorybibliography}
	\end{document}
\else

\fi

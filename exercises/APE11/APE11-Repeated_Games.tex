\documentclass{../ape}

\usepackage{../../linma2345}

\newcommand{\reels}{\mathbb{R}}
\newcommand{\X}{X}
\newcommand{\Y}{Y}

\begin{document}

\session{11}{Repeated games}

\section{The example that says it all}
The prisoner dilemma. A wonderful game where you learn that you can't be both a good player and a nice person.

Prisoner Dilemmas are typical game where two players can decide either to cooperate with each other, which implies providing a service to the other, or to defect, which implies not doing anything for the other. In that situation, it is always preferable to receive than to give. The following game is an instance of PD:
\begin{center}
\begin{tabular}{c|cc}
& C & D \\
C & 5/5 & 0/6 \\
D & 6/0 & 1/1
\end{tabular}
\end{center}
The only Nash Equilibrium of this game is $(D,D)$, that is, both players ``Defect'' instead of ``Cooperate''.

We now put ourselves in the situation where players will have to play this game again and again and again, and will investigate scenarios where cooperation will naturally occur!
\begin{enumerate}
	\item[a.] Assume that the game is going to be played exactly N times. Discuss the strategies of the players as a function of $N$. 
	\item[b.] Now, the game can potentially be repeated an infinite amount of time. At every step, first they decide on their strategy, then they play the game and receive their payoffs, and finally, a computer decides with probability $\alpha$ that there will be a next step and probability $1-\alpha$ that the game ends there.\\
	Describe this situation as a repeated game $$\Gamma(N, \Theta, (D_i, S_i, u_i)_{i \in N}, q, r).$$
	Recall that $N$ is the set of players, $\Theta$ is a set of \emph{state of the worlds} (you need to decide what it is going to be), $S_i$ is a set of signals, used by the players to decide their moves (here also, what could it be?), $u : \Theta \times D \rightarrow \reels$, $q$ is an initial condition in $\Delta( \Theta \times S)$, and $p$ describes the transitions between the states.
	\item[c.] What would be the average payoff of the players if they did the following:
		\emph{every time I can play, I just play 0.5[C]+0.5[D]}. 
\end{enumerate}

\begin{solution}
\input{1101.tex}
\end{solution}

\section{War of attrition.}
Two small grocery stores close to one another are starting to feel the effects of the supermarket that has just moved to a kilometer away. While they remain on the market, they each lose \SI{4000}{\EUR} per month. The first day of the month, just before paying rent, each grocer still on the market must decide whether or not to close his shop. If one of the two stores closes, the one who remains on the market will make a profit of \SI{2000}{\EUR} a month. In addition, it is assumed that the closure of a grocery store is final. Every grocer wants to maximize the \emph{discounted expected value} of its monthly benefit, assuming a discount factor of $\delta = .99$.
\begin{enumerate}
	\item[a.] Find an equilibrium for this situation in which the two grocers randomize between leaving or staying on the market, and this until at least one of them actually closes his store.
	\item[b.] How else could the grocers have equilibrated this situation?
	\item[c.] Now suppose that Grocer~1 was slightly larger than Grocer~2. As long as the two grocers remain on the market, Grocer~1 loses \SI{4800}{\EUR} a month while Grocer~2 loses only \SI{3600}{\EUR} a month. Conversely, if Grocer~1 was the only one on the market, he would win \SI{2800}{\EUR} a month, against \SI{1600}{\EUR} only for Grocer~2 in the same situation. Find an equilibrium for the situation in which the two grocers randomize each month between leaving the market or stay there until at least one of them actually closes its store. In this case, which of the two grocers has the highest chance to leave first?
\end{enumerate}

\nosolution

%\newpage

\newpage

\section*{Reminders}

\begin{itemize}[leftmargin=*]
\renewcommand{\labelitemi}{$\bullet$}

	\item \textbf{Equilibria of a repeated game}
	\vspace{.3cm}

	Let $N$ be the set of players and $\Theta$ be the set of states of the game. Let $D_i$ be the set of actions of player $i$, $S_i$ be the set of signals that he can receive on the state of the game, $u_i : D_i \times \Theta \rightarrow \mathbb{R}$ be his payoffs and $\tau_i : \Theta \rightarrow \Delta(D_i)$ be his strategy. Finally, let $p : D \times \Theta \rightarrow \Delta(S \times \Theta)$ be the transition function between the states.
	
	The expected $\delta$-discounted average payoff of player $i$ when the initial state is $\theta \in \Theta$ can be written as:
	\begin{align}
		\nu_i(\theta) = \sum_{d_i \in D_i} \tau_i(d_i|\theta) \, Y_i(\tau , d_i , \nu_i , \theta , \delta) \hspace{1cm}
		\label{7.2}
	\end{align}
	where $0 \leq \delta < 1$ is the discount factor and $Y_i(\tau , d_i , \nu_i , \theta , \delta)$ corresponds to what player $i$ would gain by playing the action $d_i$ in state $\theta$ and then the stationary strategy $\tau_i$, and is defined by:
	\begin{align*}
		Y_i(&\tau , d_i , \nu_i , \theta , \delta) = \\ &\sum_{d_{-i} \in D_{-i}} \Bigg( \prod_{j \in N_{-i}} \tau_j(d_j|\theta) \Bigg)\Bigg( (1-\delta)u_i(d,\theta)+\delta \sum_{\zeta \in \Theta} \sum_{s \in S}p(s, \zeta|d,\theta)\nu_i(\zeta)\Bigg).
	\end{align*}
	
	For a stationary strategy profile $\tau$ to be an equilibrium of a repeated game with complete information, no player should be able to expect to improve its payoff by changing its strategy at any stage or in any state $\theta$. This condition implies:
	\begin{align}
		\nu_i(\theta)= \max_{d_i \in D_i} Y_i(\tau , d_i , \nu_i , \theta , \delta) \hspace{1cm} \forall \theta \in \Theta.
		\label{7.3}
	\end{align}
	The strategy $\tau$ is an equilibrium of a repeated game with complete information iff there exists a bounded payoff function $\nu$ satisfying Conditions~\eqref{7.2} and~\eqref{7.3}.

	\vspace{.3cm}

\end{itemize}

\end{document}










